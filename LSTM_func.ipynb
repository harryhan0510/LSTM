{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def windows(sample_data, window_size):\n",
    "    start = 0\n",
    "    while start < sample_data.shape[1]:\n",
    "        yield start, start + window_size\n",
    "        start += (window_size)\n",
    "        \n",
    "def ts_to_supervise(data,window_size,n_var):\n",
    "    \"\"\"\n",
    "        Convert input data into LSTMs data\n",
    "        \n",
    "        args:\n",
    "        window_size : the length of time-series. All input columns must have identical length\n",
    "        n_var : number of features regardless the time-series period\n",
    "        \n",
    "        returns:\n",
    "        result_array : A 3-D LSTMs array \n",
    "        \n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    result_array = np.asarray(np.zeros((data.shape[0],window_size,n_var)))\n",
    "    for i in range(0,data.shape[0]):\n",
    "    \n",
    "        segments = None\n",
    "        \n",
    "        for (start, end) in windows(data, window_size):\n",
    "            if (data[:,start:end].shape[1]== (window_size)):\n",
    "                signal = np.asarray(data[i,start:end])\n",
    "                if segments is None:\n",
    "                    segments = signal\n",
    "                else:\n",
    "                    segments = np.vstack([segments, signal])\n",
    "        segments = segments.transpose()\n",
    "\n",
    "        result_array[i,:,:] = segments\n",
    "    return(result_array)\n",
    "\n",
    "def drop_col(columns, col_thresh, sample_x):\n",
    "    \"\"\"\n",
    "        Drop zero-inflation columns\n",
    "    \"\"\"\n",
    "    col_to_drop = []\n",
    "    zero_probs = []\n",
    "    \n",
    "    for col in columns:\n",
    "        \n",
    "        zero_prob = sum((sample_x[col] == 0).astype(int))/len(sample_x[col])\n",
    "        \n",
    "        zero_probs.append(zero_prob)\n",
    "        \n",
    "        if sample_x[col].dtype != 'int64':\n",
    "            sample_x[col].astype('int64')\n",
    "            print(\"[features]: numerize colname names:\" ,col)  \n",
    "        \n",
    "        if zero_prob > col_thresh:\n",
    "            col_to_drop.append(col)\n",
    "            print('[features]:',col, \"has been dropped with zeros more than\", zero_prob)\n",
    "    return col_to_drop, zero_probs\n",
    "\n",
    "def get_feature_name(col_list):\n",
    "    \"\"\"\n",
    "        Get the time-series features' names regardless of the time period\n",
    "    \"\"\"\n",
    "    feature_names =[]\n",
    "    for col in col_list:\n",
    "        feature_name = col.split('_')[0]\n",
    "        feature_names.append(feature_name)\n",
    "    return(feature_names)\n",
    "\n",
    "def features_extract (sample_df, ts_len):\n",
    "    \"\"\"\n",
    "        Filter time-series features from the input data and return a list of those features' names\n",
    "    \"\"\"\n",
    "    raw_features = sample_df.columns\n",
    "    fea_list = get_feature_name(raw_features)\n",
    "\n",
    "    ts_features = []\n",
    "    for col in set(fea_list):\n",
    "        if fea_list.count(col) == ts_len:\n",
    "            ts_features.append(col)\n",
    "    print('[features]: ',len(ts_features),\"time series features founded from data...\")\n",
    "\n",
    "    output_features = []\n",
    "    for feature_name in ts_features:\n",
    "        try:\n",
    "            print('[features]: generating windowed average deltas...',feature_name)\n",
    "            # get the column names\n",
    "            feats = list(sample_df.filter(regex=(feature_name+'*_[0-59]')).columns)\n",
    "            print(len(feats),\"founded\")\n",
    "        except:\n",
    "             print('[features]: Oops! features name not found...')\n",
    "        output_features.append(feats)\n",
    "    output_features = [item for sublist in output_features for item in sublist]\n",
    "    output_features = list(set(output_features))\n",
    "    return(output_features)\n",
    "\n",
    "\n",
    "def train_test_split(sample_data, valsize = 0.05, random_state = 42):\n",
    "    \n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    _, _, train_idx, val_idx = train_test_split(sample_data, sample_data.index, test_size=valsize, random_state=random_state)\n",
    "    \n",
    "    sample_data.ix[train_idx,'split'] = 'Train'\n",
    "    sample_data.ix[val_idx,'split'] = 'Validation'\n",
    "    \n",
    "    print(sample_data['split'].value_counts())\n",
    "    \n",
    "    return(sample_data)\n",
    "\n",
    "def plot_confusion_matrix(truth, predicted, labels={}, save_name='',\n",
    "                          title='Confusion Matrix', norm=1, suppress_values=False,\n",
    "                          diagonal_values=False,\n",
    "                          font_size=10,\n",
    "                          cmin=0,cmax=1,\n",
    "                          cut_off = 1\n",
    "                          ):\n",
    "    # make confusion matrix from truth and predicted for classes\n",
    "    # define the confusion matrix\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    from sklearn.metrics import classification_report\n",
    "    import numpy as np\n",
    "    \n",
    "    conf_mat = confusion_matrix(truth,predicted,labels = labels)\n",
    "    \n",
    "    #normalise\n",
    "    if norm:\n",
    "        conf_mat =  conf_mat.astype('float')/conf_mat.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    fig = plt.figure(figsize=(9,9))\n",
    "    width = np.shape(conf_mat)[1]\n",
    "    height = np.shape(conf_mat)[0]\n",
    "\n",
    "    res = plt.imshow(np.array(conf_mat), cmap=plt.cm.summer, interpolation='nearest')\n",
    "    cb = fig.colorbar(res)\n",
    "    \n",
    "    res.set_clim(cmin, cmax)\n",
    "    \n",
    "    # add number overlay\n",
    "    for i, row in enumerate(conf_mat):\n",
    "        for j, c in enumerate(row):\n",
    "            if (not suppress_values or (diagonal_values and i==j)) and c>0 :\n",
    "                cent = .1\n",
    "                if diagonal_values:\n",
    "                    cent = .3\n",
    "                \n",
    "                if norm:\n",
    "                    d = round(c,2)\n",
    "                    plt.text(j-cent, i+.0, d, fontsize=font_size)\n",
    "                else:\n",
    "                    plt.text(j-cent, i+.0, c, fontsize=font_size)\n",
    "    \n",
    "            if (i==j) and c > cut_off:\n",
    "                cent= 0.3\n",
    "                plt.text(j-cent, i+.0, 'X', fontsize=font_size)\n",
    "\n",
    "    # set axes\n",
    "    plt.xticks(range(len(labels)), labels, rotation = 60, fontsize=font_size)\n",
    "    plt.yticks(range(len(labels)), labels, rotation = 60, fontsize=font_size)\n",
    "    \n",
    "    plt.xlabel('Predicted',fontsize=font_size+4)\n",
    "    plt.ylabel('Truth',fontsize=font_size+4)\n",
    "    plt.title(title,fontsize=font_size+5)\n",
    "\n",
    "    if save_name != '':\n",
    "        plt.savefig(save_name)\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_roc(test_y, pred_y, roc_save_name = None):\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    fpr, tpr, _ = roc_curve(test_y, pred_y[:,1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    \n",
    "    if roc_save_name != '':\n",
    "        plt.savefig(roc_save_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def data_processing(data, label, columns, ts_len, col_thresh, random):\n",
    "    \"\"\"\n",
    "        Pre-process the data to LSTMs data\n",
    "        In this function, we clean the input dataset with selected columns, delete those zero-inflation columns\n",
    "        and convert the reserved to LSTMs dataset for LSTM model fitting.\n",
    "        \n",
    "        args:\n",
    "            data : directory of input data\n",
    "            label : name of the column containing respond variable\n",
    "            columns : names of the column of features (must be time series type)\n",
    "            col_thresh : a threshhold value between 0 - 1 for dropping zero-inflation columns\n",
    "            ts_len : length of time series\n",
    "            test : True/False, testing data\n",
    "            \n",
    "        returns:\n",
    "            feature_names_reserve: Features names reserved\n",
    "            feature_names_drop: Features names dropped\n",
    "            train_array: Processed training X\n",
    "            val_array: Processed validation X\n",
    "            test_array: Processed testing X\n",
    "            Y_train: Processed training Y\n",
    "            Y_val: Processed validation Y\n",
    "            Y_test: Processed testing Y\n",
    "            zero_probs: An array of precentage of zeros each columns\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    \n",
    "    # Read data\n",
    "    sample_data = pd.read_csv(data)\n",
    "    \n",
    "    # Extract outcome label\n",
    "    sample_y = sample_data[label]\n",
    "\n",
    "    # Extract data with interested features\n",
    "    sample_x = sample_data[columns]\n",
    "    \n",
    "    # Obtain columns with a lot of zero values, col_thresh is the threshold for zero percentage\n",
    "    \n",
    "    col_to_drop, zero_probs = drop_col(columns, col_thresh, sample_x)\n",
    "     \n",
    "    sample_x = sample_x.drop(col_to_drop,1)\n",
    "    \n",
    "    # Obtain dropped feature names\n",
    "    col_list = list(col_to_drop)\n",
    "    feature_names_drop = get_feature_name(col_list)     \n",
    "    print('[features]:',len(set(feature_names_drop)),\"zero-inflation features\")\n",
    "\n",
    "    # Obtain reserved feature names\n",
    "    col_list = list(sample_x.columns)\n",
    "    feature_names_reserve = get_feature_name(col_list)\n",
    "    print('[features]:',len(set(feature_names_reserve)),\"features will be included in LSTMs model\")\n",
    "    \n",
    "    x_values = sample_x.values\n",
    "#     scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "#     values = scaler.fit_transform(x_values)\n",
    "#     x_values = scaler.transform(values)\n",
    "    \n",
    "    # Train/test split\n",
    "    sample_data = train_test_split(sample_data, valsize = 0.05, random_state = random)\n",
    "\n",
    "    # Obtain training data  \n",
    "    X_train = x_values[sample_data.split=='Train']\n",
    "    Y_train = np.asarray(pd.get_dummies(sample_y[sample_data.split=='Train'] ), dtype=np.int8)\n",
    "    # convert training data to LSTM data\n",
    "    train_array = ts_to_supervise(X_train,ts_len,len(set(feature_names_reserve)))\n",
    "    \n",
    "    # Obtain validation data    \n",
    "    X_val = x_values[sample_data.split=='Validation']\n",
    "    Y_val = np.asarray(pd.get_dummies(sample_y[sample_data.split=='Validation'] ), dtype=np.int8) \n",
    "    val_array = ts_to_supervise(X_val,ts_len,len(set(feature_names_reserve)))\n",
    "    \n",
    "    full_x_array = ts_to_supervise(x_values,ts_len,len(set(feature_names_reserve)))\n",
    "    \n",
    "    return full_x_array, feature_names_reserve, feature_names_drop, train_array, val_array, Y_train, Y_val, zero_probs\n",
    "\n",
    "def fit_lstm(train_x, train_y, val_x, val_y, epochs, batch_size):\n",
    "    \"\"\"\n",
    "        Fit LSTMs model\n",
    "        In this function, we fit the LSTMs model with customized parameters\n",
    "        args:\n",
    "            train_x : training features\n",
    "            train_y : training y\n",
    "            val_x : validation features\n",
    "            val_y : validation y\n",
    "            epochs : number of epoch\n",
    "            batch_size : batch size\n",
    "        returns:\n",
    "            lstm_model : fitted LSTM model\n",
    "    \"\"\"\n",
    "    from keras.layers import Dense\n",
    "    from keras.layers import LSTM, Dropout\n",
    "    from keras import optimizers\n",
    "    from keras.models import Sequential\n",
    "    \n",
    "    # Initialize model    \n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128,input_shape=(train_x.shape[1], train_x.shape[2]), return_sequences=False))\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    opt = optimizers.adam(lr = 0.001)\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    # 128 is the number of hidden units sent forward to the next time step, a.k.a, dimensionality of the output space.\n",
    "    # input_shape is the dimensionality of the input needed to claim in the first layer.\n",
    "    # return sequences is a boolean indicating whether to return the last output in the output sequence, or the full sequence.\n",
    "#     model.add(LSTM(20, input_shape=(train_x.shape[1], train_x.shape[2]),return_sequences=True))\n",
    "#     model.add(LSTM(10, return_sequences=True))\n",
    "#     model.add(LSTM(5, return_sequences=False))\n",
    "\n",
    "    # regular densely-connected NN layer with 2 dimensionality of output space.\n",
    "#     model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    # configure the model for training\n",
    "    # loss is a loss function used in model fitting, we use categorical_crossentropy because we are predicting categorical outcome\n",
    "    # optimizer is determined for compiping a Keras model\n",
    "    # matrics is for evaluation matrics and here we use 'accuracy'\n",
    "#     opt = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "#     model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "    # summarize the model in table\n",
    "#     model.summary()\n",
    "    \n",
    "    # fit model\n",
    "    lstm_model = model.fit(train_x, train_y, epochs=epochs, batch_size=batch_size, validation_data= (val_x, val_y), verbose=2\n",
    "                           , shuffle=False)\n",
    "    \n",
    "    return(lstm_model)\n",
    "def val_lstm(sample_df, model, x_values, y_values, batch_size, cutoff = 0.5, roc_save_name='', cm_save_name = '', result_csv_save_name = ''):\n",
    "    \"\"\"\n",
    "        Validate LSTMs model\n",
    "        In this function, we validate our fitted LSTMs model with some evaluation plots\n",
    "        args:\n",
    "            model : fitted LSTMs model\n",
    "            test_x : testing features\n",
    "            test_y : testing y\n",
    "            batch_size : batch size\n",
    "            cutoff : predicted y = 1, if Predicted probability > 0.5, else, 0 \n",
    "            roc_save_name : name of saved roc plot\n",
    "            cm_save_name : name of saved confusion matrix plot\n",
    "            result_csv_save_name : name of saved result csv file\n",
    "        returns:\n",
    "            result_df : result csv with predicted probability, category, etc.\n",
    "    \"\"\"\n",
    "   \n",
    "    # get fitted y\n",
    "    pred_y = model.predict(x_values, batch_size=batch_size, verbose=2)\n",
    "    \n",
    "    \n",
    "    # Combine data\n",
    "    result_df = pd.concat([sample_df['serial'],sample_df['split'],pd.Series(y_values, name = 'true_y'),pd.Series(pred_y[:,1], name = 'pred_y')],axis = 1)\n",
    "    \n",
    "    # Write csv\n",
    "    result_df.to_csv(result_csv_save_name, sep = '\\t')\n",
    "    \n",
    "    # draw roc\n",
    "    plot_roc(y_values, pred_y, roc_save_name = roc_save_name)\n",
    "    \n",
    "    # draw confusion matrix\n",
    "    ypred = [[ 1 if x > cutoff else 0 ]for x in pred_y[:,1]]\n",
    "    plot_confusion_matrix(truth = y_values, predicted = ypred,labels=[0,1],save_name=cm_save_name)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[features]:  44 time series features founded from data...\n",
      "[features]: generating windowed average deltas... MinPumpPerfLevel\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... dPMin\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... HeparinConcentration\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MeanMotorSpeed\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MaxPurgePressure\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MeanTargetPumpPerfLvl\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... dPRange\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... RangePlacement\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... TargetPumpFlow\n",
      "240 founded\n",
      "[features]: generating windowed average deltas... MinMotorSpeed\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MaxTargetPumpFlow\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... PumpPosition\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MinPurgeFlowrate\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... PumpRunTime\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... dPMax\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MaxPumpFlow\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... RangeMotorSpeed\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... RangeMotorCurrent\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MeanPlacement\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MinPlacement\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MaxTargetPumpPerfLvl\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MinMotorCurrent\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... TargetPumpPerfLvl\n",
      "240 founded\n",
      "[features]: generating windowed average deltas... MeanTargetPumpFlow\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MinTargetPumpFlow\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MaxLVP\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MinPurgePressure\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... GlucoseConcentration\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MaxPurgeFlowrate\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MinPumpFlow\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MaxPumpPerfLevel\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MeanPumpFlow\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MaxPlacement\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MaxMotorCurrent\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... RangePumpFlow\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MeanPurgeFlowrate\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... NCPO\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MeanPumpPerfLevel\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... dPMean\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MeanMotorCurrent\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MeanPurgePressure\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MaxMotorSpeed\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... MinTargetPumpPerfLvl\n",
      "60 founded\n",
      "[features]: generating windowed average deltas... ACPO\n",
      "60 founded\n"
     ]
    }
   ],
   "source": [
    "# Noticed that the input features name in data_processing function must be exactly the same as the column name in data.\n",
    "# i.e. \"MaxExternalPressure_00\", \"MaxExternalPressure_01\", ... \"MaxExternalPressure_59\", etc..\n",
    "# Here we manually extract the interested features name ('x_name' below) before running the function.\n",
    "import pandas as pd\n",
    "sample_df = pd.read_csv(\"wide_table_val.csv\")\n",
    "features_list = features_extract(sample_df, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MaxTargetPumpFlow_26 has been dropped with zeros more than 0.9861111111111112\n",
      "numerize colname names: dPMean_22\n",
      "MeanTargetPumpFlow_03 has been dropped with zeros more than 0.985663082437276\n",
      "TargetPumpFlow_34 has been dropped with zeros more than 0.839605734767025\n",
      "numerize colname names: dPMean_57\n",
      "MinTargetPumpFlow_32 has been dropped with zeros more than 0.9864097968936678\n",
      "MinTargetPumpFlow_37 has been dropped with zeros more than 0.9859617682198327\n",
      "MeanTargetPumpFlow_40 has been dropped with zeros more than 0.9855137395459976\n",
      "MaxTargetPumpFlow_12 has been dropped with zeros more than 0.9858124253285544\n",
      "MaxTargetPumpFlow_28 has been dropped with zeros more than 0.9858124253285544\n",
      "numerize colname names: dPMean_40\n",
      "MeanTargetPumpFlow_49 has been dropped with zeros more than 0.9835722819593787\n",
      "TargetPumpFlow_20 has been dropped with zeros more than 0.8322879330943848\n",
      "MinTargetPumpFlow_50 has been dropped with zeros more than 0.9840203106332138\n",
      "TargetPumpFlow_29 has been dropped with zeros more than 0.8369175627240143\n",
      "numerize colname names: dPMean_09\n",
      "numerize colname names: dPMean_26\n",
      "MaxTargetPumpFlow_41 has been dropped with zeros more than 0.985663082437276\n",
      "MinTargetPumpFlow_25 has been dropped with zeros more than 0.9865591397849462\n",
      "MaxTargetPumpFlow_50 has been dropped with zeros more than 0.9834229390681004\n",
      "MeanTargetPumpFlow_21 has been dropped with zeros more than 0.9861111111111112\n",
      "numerize colname names: dPMean_28\n",
      "MeanTargetPumpFlow_13 has been dropped with zeros more than 0.9864097968936678\n",
      "TargetPumpFlow_22 has been dropped with zeros more than 0.8327359617682198\n",
      "MaxTargetPumpFlow_47 has been dropped with zeros more than 0.984468339307049\n",
      "MinTargetPumpFlow_59 has been dropped with zeros more than 0.9855137395459976\n",
      "numerize colname names: dPMean_12\n",
      "MinTargetPumpFlow_40 has been dropped with zeros more than 0.9859617682198327\n",
      "TargetPumpFlow_01 has been dropped with zeros more than 0.8252688172043011\n",
      "MaxTargetPumpFlow_09 has been dropped with zeros more than 0.9861111111111112\n",
      "numerize colname names: dPMean_31\n",
      "numerize colname names: dPMean_18\n",
      "MeanTargetPumpFlow_53 has been dropped with zeros more than 0.9828255675029869\n",
      "MeanTargetPumpFlow_33 has been dropped with zeros more than 0.9861111111111112\n",
      "numerize colname names: dPMean_01\n",
      "MinTargetPumpFlow_42 has been dropped with zeros more than 0.9858124253285544\n",
      "numerize colname names: dPMean_35\n",
      "MinTargetPumpFlow_49 has been dropped with zeros more than 0.9847670250896058\n",
      "MaxTargetPumpFlow_19 has been dropped with zeros more than 0.9859617682198327\n",
      "MaxTargetPumpFlow_08 has been dropped with zeros more than 0.985663082437276\n",
      "TargetPumpFlow_27 has been dropped with zeros more than 0.8355734767025089\n",
      "MinTargetPumpFlow_52 has been dropped with zeros more than 0.9835722819593787\n",
      "numerize colname names: dPMean_21\n",
      "MaxTargetPumpFlow_38 has been dropped with zeros more than 0.9855137395459976\n",
      "MeanTargetPumpFlow_38 has been dropped with zeros more than 0.9855137395459976\n",
      "MeanTargetPumpFlow_01 has been dropped with zeros more than 0.9855137395459976\n",
      "TargetPumpFlow_12 has been dropped with zeros more than 0.8303464755077659\n",
      "MinTargetPumpFlow_33 has been dropped with zeros more than 0.9862604540023895\n",
      "TargetPumpFlow_35 has been dropped with zeros more than 0.8397550776583035\n",
      "MeanTargetPumpFlow_48 has been dropped with zeros more than 0.9843189964157706\n",
      "TargetPumpFlow_37 has been dropped with zeros more than 0.840352449223417\n",
      "TargetPumpFlow_28 has been dropped with zeros more than 0.8364695340501792\n",
      "numerize colname names: dPMean_41\n",
      "TargetPumpFlow_05 has been dropped with zeros more than 0.8270609318996416\n",
      "TargetPumpFlow_44 has been dropped with zeros more than 0.8434886499402628\n",
      "numerize colname names: dPMean_59\n",
      "MinTargetPumpFlow_55 has been dropped with zeros more than 0.983273596176822\n",
      "numerize colname names: dPMean_39\n",
      "numerize colname names: dPMean_56\n",
      "MaxTargetPumpFlow_44 has been dropped with zeros more than 0.9850657108721624\n",
      "TargetPumpFlow_06 has been dropped with zeros more than 0.8269115890083633\n",
      "MinTargetPumpFlow_23 has been dropped with zeros more than 0.9864097968936678\n",
      "MinTargetPumpFlow_30 has been dropped with zeros more than 0.9864097968936678\n",
      "MinTargetPumpFlow_28 has been dropped with zeros more than 0.9864097968936678\n",
      "TargetPumpFlow_42 has been dropped with zeros more than 0.8425925925925926\n",
      "TargetPumpFlow_30 has been dropped with zeros more than 0.8378136200716846\n",
      "MeanTargetPumpFlow_37 has been dropped with zeros more than 0.9858124253285544\n",
      "TargetPumpFlow_38 has been dropped with zeros more than 0.8406511350059738\n",
      "MaxTargetPumpFlow_55 has been dropped with zeros more than 0.9823775388291517\n",
      "numerize colname names: dPMean_33\n",
      "MeanTargetPumpFlow_34 has been dropped with zeros more than 0.9859617682198327\n",
      "MaxTargetPumpFlow_31 has been dropped with zeros more than 0.9861111111111112\n",
      "TargetPumpFlow_40 has been dropped with zeros more than 0.8424432497013142\n",
      "MaxTargetPumpFlow_36 has been dropped with zeros more than 0.985663082437276\n",
      "TargetPumpFlow_00 has been dropped with zeros more than 0.8254181600955794\n",
      "MaxTargetPumpFlow_23 has been dropped with zeros more than 0.9859617682198327\n",
      "MinTargetPumpFlow_27 has been dropped with zeros more than 0.9861111111111112\n",
      "numerize colname names: dPMean_42\n",
      "TargetPumpFlow_11 has been dropped with zeros more than 0.8298984468339307\n",
      "MeanTargetPumpFlow_44 has been dropped with zeros more than 0.9850657108721624\n",
      "TargetPumpFlow_51 has been dropped with zeros more than 0.8484169653524493\n",
      "TargetPumpFlow_48 has been dropped with zeros more than 0.8461768219832736\n",
      "numerize colname names: dPMean_07\n",
      "MaxTargetPumpFlow_05 has been dropped with zeros more than 0.9859617682198327\n",
      "MeanTargetPumpFlow_43 has been dropped with zeros more than 0.9852150537634409\n",
      "TargetPumpFlow_24 has been dropped with zeros more than 0.8333333333333334\n",
      "TargetPumpFlow_15 has been dropped with zeros more than 0.8309438470728794\n",
      "MinTargetPumpFlow_10 has been dropped with zeros more than 0.9862604540023895\n",
      "MaxTargetPumpFlow_22 has been dropped with zeros more than 0.9861111111111112\n",
      "MaxTargetPumpFlow_04 has been dropped with zeros more than 0.985663082437276\n",
      "MinTargetPumpFlow_22 has been dropped with zeros more than 0.9862604540023895\n",
      "MeanTargetPumpFlow_32 has been dropped with zeros more than 0.9859617682198327\n",
      "MinTargetPumpFlow_08 has been dropped with zeros more than 0.9864097968936678\n",
      "MeanTargetPumpFlow_56 has been dropped with zeros more than 0.9823775388291517\n",
      "numerize colname names: dPMean_44\n",
      "MeanTargetPumpFlow_51 has been dropped with zeros more than 0.983273596176822\n",
      "MaxTargetPumpFlow_58 has been dropped with zeros more than 0.980884109916368\n",
      "MeanTargetPumpFlow_16 has been dropped with zeros more than 0.9862604540023895\n",
      "numerize colname names: dPMean_54\n",
      "MaxTargetPumpFlow_32 has been dropped with zeros more than 0.9859617682198327\n",
      "MeanTargetPumpFlow_45 has been dropped with zeros more than 0.9849163679808841\n",
      "MeanTargetPumpFlow_19 has been dropped with zeros more than 0.9861111111111112\n",
      "TargetPumpFlow_07 has been dropped with zeros more than 0.8278076463560334\n",
      "MeanTargetPumpFlow_05 has been dropped with zeros more than 0.9859617682198327\n",
      "numerize colname names: dPMean_19\n",
      "numerize colname names: dPMean_13\n",
      "MaxTargetPumpFlow_33 has been dropped with zeros more than 0.9861111111111112\n",
      "TargetPumpFlow_57 has been dropped with zeros more than 0.855884109916368\n",
      "numerize colname names: dPMean_04\n",
      "MeanTargetPumpFlow_11 has been dropped with zeros more than 0.9861111111111112\n",
      "MinTargetPumpFlow_13 has been dropped with zeros more than 0.9865591397849462\n",
      "MinTargetPumpFlow_17 has been dropped with zeros more than 0.9862604540023895\n",
      "TargetPumpFlow_23 has been dropped with zeros more than 0.8327359617682198\n",
      "MaxTargetPumpFlow_16 has been dropped with zeros more than 0.9862604540023895\n",
      "numerize colname names: dPMean_48\n",
      "MinTargetPumpFlow_24 has been dropped with zeros more than 0.9865591397849462\n",
      "MinTargetPumpFlow_56 has been dropped with zeros more than 0.9834229390681004\n",
      "numerize colname names: dPMean_16\n",
      "MeanTargetPumpFlow_27 has been dropped with zeros more than 0.9859617682198327\n",
      "MinTargetPumpFlow_51 has been dropped with zeros more than 0.9841696535244923\n",
      "TargetPumpFlow_10 has been dropped with zeros more than 0.8293010752688172\n",
      "TargetPumpFlow_47 has been dropped with zeros more than 0.8452807646356033\n",
      "numerize colname names: dPMean_10\n",
      "TargetPumpFlow_33 has been dropped with zeros more than 0.8397550776583035\n",
      "numerize colname names: dPMean_25\n",
      "MinTargetPumpFlow_29 has been dropped with zeros more than 0.9862604540023895\n",
      "numerize colname names: dPMean_00\n",
      "MeanTargetPumpFlow_36 has been dropped with zeros more than 0.985663082437276\n",
      "MeanTargetPumpFlow_08 has been dropped with zeros more than 0.985663082437276\n",
      "MinTargetPumpFlow_58 has been dropped with zeros more than 0.9828255675029869\n",
      "MinTargetPumpFlow_39 has been dropped with zeros more than 0.9861111111111112\n",
      "MinTargetPumpFlow_04 has been dropped with zeros more than 0.9859617682198327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MeanTargetPumpFlow_28 has been dropped with zeros more than 0.9858124253285544\n",
      "MinTargetPumpFlow_12 has been dropped with zeros more than 0.986857825567503\n",
      "TargetPumpFlow_08 has been dropped with zeros more than 0.828853046594982\n",
      "MaxTargetPumpFlow_00 has been dropped with zeros more than 0.9853643966547192\n",
      "TargetPumpFlow_14 has been dropped with zeros more than 0.8303464755077659\n",
      "MinTargetPumpFlow_53 has been dropped with zeros more than 0.9837216248506571\n",
      "numerize colname names: dPMean_58\n",
      "MinTargetPumpFlow_03 has been dropped with zeros more than 0.985663082437276\n",
      "numerize colname names: dPMean_20\n",
      "MinTargetPumpFlow_16 has been dropped with zeros more than 0.9864097968936678\n",
      "numerize colname names: dPMean_05\n",
      "TargetPumpFlow_02 has been dropped with zeros more than 0.8258661887694145\n",
      "numerize colname names: dPMean_23\n",
      "numerize colname names: dPMean_34\n",
      "numerize colname names: dPMean_38\n",
      "MeanTargetPumpFlow_58 has been dropped with zeros more than 0.980884109916368\n",
      "MaxTargetPumpFlow_37 has been dropped with zeros more than 0.9858124253285544\n",
      "MaxTargetPumpFlow_02 has been dropped with zeros more than 0.9855137395459976\n",
      "MeanTargetPumpFlow_30 has been dropped with zeros more than 0.9861111111111112\n",
      "MeanTargetPumpFlow_55 has been dropped with zeros more than 0.9823775388291517\n",
      "MinTargetPumpFlow_19 has been dropped with zeros more than 0.9865591397849462\n",
      "MeanTargetPumpFlow_12 has been dropped with zeros more than 0.9858124253285544\n",
      "MinTargetPumpFlow_43 has been dropped with zeros more than 0.9853643966547192\n",
      "MeanTargetPumpFlow_07 has been dropped with zeros more than 0.9859617682198327\n",
      "numerize colname names: dPMean_06\n",
      "numerize colname names: dPMean_32\n",
      "MinTargetPumpFlow_00 has been dropped with zeros more than 0.9859617682198327\n",
      "MaxTargetPumpFlow_29 has been dropped with zeros more than 0.9861111111111112\n",
      "TargetPumpFlow_58 has been dropped with zeros more than 0.8581242532855436\n",
      "TargetPumpFlow_09 has been dropped with zeros more than 0.8294504181600956\n",
      "MaxTargetPumpFlow_03 has been dropped with zeros more than 0.985663082437276\n",
      "MaxTargetPumpFlow_53 has been dropped with zeros more than 0.9828255675029869\n",
      "TargetPumpFlow_21 has been dropped with zeros more than 0.8324372759856631\n",
      "MeanTargetPumpFlow_00 has been dropped with zeros more than 0.9853643966547192\n",
      "MeanTargetPumpFlow_54 has been dropped with zeros more than 0.9828255675029869\n",
      "TargetPumpFlow_18 has been dropped with zeros more than 0.831989247311828\n",
      "numerize colname names: dPMean_27\n",
      "MeanTargetPumpFlow_47 has been dropped with zeros more than 0.984468339307049\n",
      "numerize colname names: dPMean_51\n",
      "MinTargetPumpFlow_46 has been dropped with zeros more than 0.9850657108721624\n",
      "MeanTargetPumpFlow_52 has been dropped with zeros more than 0.9829749103942652\n",
      "MeanTargetPumpFlow_41 has been dropped with zeros more than 0.985663082437276\n",
      "numerize colname names: dPMean_29\n",
      "TargetPumpFlow_03 has been dropped with zeros more than 0.8258661887694145\n",
      "MaxTargetPumpFlow_24 has been dropped with zeros more than 0.9859617682198327\n",
      "MaxTargetPumpFlow_39 has been dropped with zeros more than 0.9852150537634409\n",
      "MaxTargetPumpFlow_07 has been dropped with zeros more than 0.9859617682198327\n",
      "MeanTargetPumpFlow_22 has been dropped with zeros more than 0.9861111111111112\n",
      "numerize colname names: dPMean_55\n",
      "MinTargetPumpFlow_48 has been dropped with zeros more than 0.9849163679808841\n",
      "numerize colname names: dPMean_15\n",
      "MeanTargetPumpFlow_14 has been dropped with zeros more than 0.9862604540023895\n",
      "MaxTargetPumpFlow_40 has been dropped with zeros more than 0.9855137395459976\n",
      "MeanTargetPumpFlow_15 has been dropped with zeros more than 0.9864097968936678\n",
      "MeanTargetPumpFlow_25 has been dropped with zeros more than 0.9864097968936678\n",
      "TargetPumpFlow_56 has been dropped with zeros more than 0.8536439665471923\n",
      "numerize colname names: dPMean_11\n",
      "MeanTargetPumpFlow_09 has been dropped with zeros more than 0.9861111111111112\n",
      "TargetPumpFlow_52 has been dropped with zeros more than 0.8490143369175627\n",
      "MinTargetPumpFlow_41 has been dropped with zeros more than 0.9858124253285544\n",
      "MaxTargetPumpFlow_54 has been dropped with zeros more than 0.9828255675029869\n",
      "MaxTargetPumpFlow_52 has been dropped with zeros more than 0.9829749103942652\n",
      "MinTargetPumpFlow_54 has been dropped with zeros more than 0.9835722819593787\n",
      "MaxTargetPumpFlow_34 has been dropped with zeros more than 0.9859617682198327\n",
      "MeanTargetPumpFlow_57 has been dropped with zeros more than 0.9816308243727598\n",
      "MeanTargetPumpFlow_04 has been dropped with zeros more than 0.985663082437276\n",
      "MeanTargetPumpFlow_29 has been dropped with zeros more than 0.9861111111111112\n",
      "MaxTargetPumpFlow_11 has been dropped with zeros more than 0.9861111111111112\n",
      "TargetPumpFlow_16 has been dropped with zeros more than 0.8313918757467145\n",
      "MeanTargetPumpFlow_59 has been dropped with zeros more than 0.9802867383512545\n",
      "TargetPumpFlow_39 has been dropped with zeros more than 0.8416965352449224\n",
      "numerize colname names: dPMean_03\n",
      "MaxTargetPumpFlow_57 has been dropped with zeros more than 0.9816308243727598\n",
      "numerize colname names: dPMean_30\n",
      "MinTargetPumpFlow_31 has been dropped with zeros more than 0.9864097968936678\n",
      "MaxTargetPumpFlow_43 has been dropped with zeros more than 0.9850657108721624\n",
      "MeanTargetPumpFlow_26 has been dropped with zeros more than 0.9861111111111112\n",
      "MinTargetPumpFlow_06 has been dropped with zeros more than 0.9859617682198327\n",
      "numerize colname names: dPMean_47\n",
      "MaxTargetPumpFlow_25 has been dropped with zeros more than 0.9864097968936678\n",
      "MaxTargetPumpFlow_35 has been dropped with zeros more than 0.9859617682198327\n",
      "numerize colname names: dPMean_08\n",
      "MeanTargetPumpFlow_35 has been dropped with zeros more than 0.9859617682198327\n",
      "MaxTargetPumpFlow_10 has been dropped with zeros more than 0.9859617682198327\n",
      "MeanTargetPumpFlow_50 has been dropped with zeros more than 0.9834229390681004\n",
      "TargetPumpFlow_59 has been dropped with zeros more than 0.8856033452807647\n",
      "MaxTargetPumpFlow_01 has been dropped with zeros more than 0.9855137395459976\n",
      "numerize colname names: dPMean_24\n",
      "numerize colname names: dPMean_49\n",
      "TargetPumpFlow_43 has been dropped with zeros more than 0.8431899641577061\n",
      "MinTargetPumpFlow_02 has been dropped with zeros more than 0.9858124253285544\n",
      "MinTargetPumpFlow_34 has been dropped with zeros more than 0.9865591397849462\n",
      "MinTargetPumpFlow_20 has been dropped with zeros more than 0.9865591397849462\n",
      "numerize colname names: dPMean_53\n",
      "TargetPumpFlow_17 has been dropped with zeros more than 0.8315412186379928\n",
      "MaxTargetPumpFlow_46 has been dropped with zeros more than 0.9849163679808841\n",
      "MinTargetPumpFlow_09 has been dropped with zeros more than 0.9864097968936678\n",
      "MeanTargetPumpFlow_46 has been dropped with zeros more than 0.9849163679808841\n",
      "MaxTargetPumpFlow_42 has been dropped with zeros more than 0.9852150537634409\n",
      "MeanTargetPumpFlow_23 has been dropped with zeros more than 0.9859617682198327\n",
      "MaxTargetPumpFlow_18 has been dropped with zeros more than 0.9861111111111112\n",
      "MaxTargetPumpFlow_56 has been dropped with zeros more than 0.9823775388291517\n",
      "numerize colname names: dPMean_52\n",
      "MinTargetPumpFlow_47 has been dropped with zeros more than 0.9847670250896058\n",
      "MeanTargetPumpFlow_20 has been dropped with zeros more than 0.9861111111111112\n",
      "MaxTargetPumpFlow_06 has been dropped with zeros more than 0.9859617682198327\n",
      "MinTargetPumpFlow_01 has been dropped with zeros more than 0.9858124253285544\n",
      "TargetPumpFlow_46 has been dropped with zeros more than 0.8446833930704899\n",
      "MaxTargetPumpFlow_45 has been dropped with zeros more than 0.9849163679808841\n",
      "numerize colname names: dPMean_37\n",
      "numerize colname names: dPMean_02\n",
      "MeanTargetPumpFlow_02 has been dropped with zeros more than 0.9855137395459976\n",
      "MaxTargetPumpFlow_27 has been dropped with zeros more than 0.9859617682198327\n",
      "MeanTargetPumpFlow_31 has been dropped with zeros more than 0.9861111111111112\n",
      "MaxTargetPumpFlow_21 has been dropped with zeros more than 0.9861111111111112\n",
      "MinTargetPumpFlow_44 has been dropped with zeros more than 0.9853643966547192\n",
      "MeanTargetPumpFlow_17 has been dropped with zeros more than 0.9859617682198327\n",
      "MinTargetPumpFlow_35 has been dropped with zeros more than 0.9861111111111112\n",
      "MinTargetPumpFlow_45 has been dropped with zeros more than 0.9853643966547192\n",
      "TargetPumpFlow_19 has been dropped with zeros more than 0.8322879330943848\n",
      "TargetPumpFlow_50 has been dropped with zeros more than 0.8466248506571087\n",
      "MaxTargetPumpFlow_20 has been dropped with zeros more than 0.9861111111111112\n",
      "MeanTargetPumpFlow_24 has been dropped with zeros more than 0.9859617682198327\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinTargetPumpFlow_14 has been dropped with zeros more than 0.9864097968936678\n",
      "numerize colname names: dPMean_14\n",
      "numerize colname names: dPMean_43\n",
      "MinTargetPumpFlow_38 has been dropped with zeros more than 0.9861111111111112\n",
      "MaxTargetPumpFlow_14 has been dropped with zeros more than 0.9862604540023895\n",
      "numerize colname names: dPMean_45\n",
      "MaxTargetPumpFlow_17 has been dropped with zeros more than 0.9859617682198327\n",
      "MinTargetPumpFlow_15 has been dropped with zeros more than 0.9864097968936678\n",
      "numerize colname names: dPMean_50\n",
      "TargetPumpFlow_25 has been dropped with zeros more than 0.8337813620071685\n",
      "MinTargetPumpFlow_05 has been dropped with zeros more than 0.9861111111111112\n",
      "MinTargetPumpFlow_07 has been dropped with zeros more than 0.9861111111111112\n",
      "MinTargetPumpFlow_36 has been dropped with zeros more than 0.9862604540023895\n",
      "TargetPumpFlow_13 has been dropped with zeros more than 0.8304958183990442\n",
      "MaxTargetPumpFlow_59 has been dropped with zeros more than 0.9801373954599761\n",
      "TargetPumpFlow_54 has been dropped with zeros more than 0.8512544802867383\n",
      "MeanTargetPumpFlow_18 has been dropped with zeros more than 0.9861111111111112\n",
      "TargetPumpFlow_41 has been dropped with zeros more than 0.8421445639187575\n",
      "numerize colname names: dPMean_46\n",
      "numerize colname names: dPMean_17\n",
      "numerize colname names: dPMean_36\n",
      "TargetPumpFlow_32 has been dropped with zeros more than 0.8384109916367981\n",
      "MaxTargetPumpFlow_30 has been dropped with zeros more than 0.9861111111111112\n",
      "MinTargetPumpFlow_11 has been dropped with zeros more than 0.9865591397849462\n",
      "MaxTargetPumpFlow_13 has been dropped with zeros more than 0.9864097968936678\n",
      "MaxTargetPumpFlow_49 has been dropped with zeros more than 0.9835722819593787\n",
      "MinTargetPumpFlow_57 has been dropped with zeros more than 0.983273596176822\n",
      "MeanTargetPumpFlow_10 has been dropped with zeros more than 0.9859617682198327\n",
      "TargetPumpFlow_36 has been dropped with zeros more than 0.8400537634408602\n",
      "MaxTargetPumpFlow_15 has been dropped with zeros more than 0.9864097968936678\n",
      "MaxTargetPumpFlow_48 has been dropped with zeros more than 0.9843189964157706\n",
      "TargetPumpFlow_26 has been dropped with zeros more than 0.834378733572282\n",
      "TargetPumpFlow_55 has been dropped with zeros more than 0.852299880525687\n",
      "TargetPumpFlow_53 has been dropped with zeros more than 0.8500597371565114\n",
      "MinTargetPumpFlow_26 has been dropped with zeros more than 0.9864097968936678\n",
      "MeanTargetPumpFlow_06 has been dropped with zeros more than 0.9859617682198327\n",
      "MinTargetPumpFlow_21 has been dropped with zeros more than 0.9864097968936678\n",
      "MeanTargetPumpFlow_39 has been dropped with zeros more than 0.9852150537634409\n",
      "MinTargetPumpFlow_18 has been dropped with zeros more than 0.9862604540023895\n",
      "TargetPumpFlow_45 has been dropped with zeros more than 0.8445340501792115\n",
      "MeanTargetPumpFlow_42 has been dropped with zeros more than 0.9852150537634409\n",
      "MaxTargetPumpFlow_51 has been dropped with zeros more than 0.983273596176822\n",
      "TargetPumpFlow_31 has been dropped with zeros more than 0.8384109916367981\n",
      "TargetPumpFlow_49 has been dropped with zeros more than 0.8466248506571087\n",
      "TargetPumpFlow_04 has been dropped with zeros more than 0.8263142174432497\n",
      "[features]: 4 zero-inflation features\n",
      "[features]: 40 features will be included in LSTMs model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\harryhan\\Anaconda\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:105: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train         6361\n",
      "Validation     335\n",
      "Name: split, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Feature processing\n",
    "full_array_x, feature_names_reserve, feature_names_drop, X_train, X_val, Y_train, Y_val, zero_per = data_processing(data = 'wide_table_val.csv', label = 'y', columns = features_list, ts_len = 60, col_thresh = 0.5, random = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 128)               86528     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 86,786\n",
      "Trainable params: 86,786\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 6361 samples, validate on 335 samples\n",
      "Epoch 1/50\n",
      " - 7s - loss: 0.6189 - acc: 0.6744 - val_loss: 0.4659 - val_acc: 0.7612\n",
      "Epoch 2/50\n",
      " - 6s - loss: 0.5025 - acc: 0.7645 - val_loss: 0.4727 - val_acc: 0.7672\n",
      "Epoch 3/50\n",
      " - 6s - loss: 0.4940 - acc: 0.7698 - val_loss: 0.4567 - val_acc: 0.8090\n",
      "Epoch 4/50\n",
      " - 6s - loss: 0.4832 - acc: 0.7753 - val_loss: 0.4333 - val_acc: 0.8000\n",
      "Epoch 5/50\n",
      " - 6s - loss: 0.4819 - acc: 0.7772 - val_loss: 0.4611 - val_acc: 0.7970\n",
      "Epoch 6/50\n",
      " - 6s - loss: 0.4849 - acc: 0.7744 - val_loss: 0.4634 - val_acc: 0.8000\n",
      "Epoch 7/50\n",
      " - 6s - loss: 0.4818 - acc: 0.7807 - val_loss: 0.4876 - val_acc: 0.7672\n",
      "Epoch 8/50\n",
      " - 6s - loss: 0.4830 - acc: 0.7821 - val_loss: 0.4620 - val_acc: 0.7881\n",
      "Epoch 9/50\n",
      " - 6s - loss: 0.4885 - acc: 0.7706 - val_loss: 0.4491 - val_acc: 0.7791\n",
      "Epoch 10/50\n",
      " - 6s - loss: 0.4828 - acc: 0.7742 - val_loss: 0.4637 - val_acc: 0.7642\n",
      "Epoch 11/50\n",
      " - 6s - loss: 0.4938 - acc: 0.7654 - val_loss: 0.4843 - val_acc: 0.7791\n",
      "Epoch 12/50\n",
      " - 6s - loss: 0.4974 - acc: 0.7742 - val_loss: 0.4687 - val_acc: 0.7940\n",
      "Epoch 13/50\n",
      " - 6s - loss: 0.4949 - acc: 0.7702 - val_loss: 0.4553 - val_acc: 0.7791\n",
      "Epoch 14/50\n",
      " - 6s - loss: 0.4920 - acc: 0.7700 - val_loss: 0.4665 - val_acc: 0.7791\n",
      "Epoch 15/50\n",
      " - 6s - loss: 0.5066 - acc: 0.7632 - val_loss: 0.4820 - val_acc: 0.7940\n",
      "Epoch 16/50\n",
      " - 6s - loss: 0.5168 - acc: 0.7548 - val_loss: 0.4946 - val_acc: 0.7433\n",
      "Epoch 17/50\n",
      " - 6s - loss: 0.5349 - acc: 0.7478 - val_loss: 0.5223 - val_acc: 0.7672\n",
      "Epoch 18/50\n",
      " - 6s - loss: 0.5069 - acc: 0.7620 - val_loss: 0.4664 - val_acc: 0.7642\n",
      "Epoch 19/50\n",
      " - 6s - loss: 0.4906 - acc: 0.7708 - val_loss: 0.4526 - val_acc: 0.7791\n",
      "Epoch 20/50\n",
      " - 6s - loss: 0.5014 - acc: 0.7665 - val_loss: 0.4613 - val_acc: 0.7851\n",
      "Epoch 21/50\n",
      " - 6s - loss: 0.4885 - acc: 0.7694 - val_loss: 0.4863 - val_acc: 0.7582\n",
      "Epoch 22/50\n",
      " - 6s - loss: 0.4975 - acc: 0.7728 - val_loss: 0.4689 - val_acc: 0.7731\n",
      "Epoch 23/50\n",
      " - 6s - loss: 0.4921 - acc: 0.7717 - val_loss: 0.4630 - val_acc: 0.7582\n",
      "Epoch 24/50\n",
      " - 6s - loss: 0.4867 - acc: 0.7788 - val_loss: 0.4494 - val_acc: 0.7791\n",
      "Epoch 25/50\n",
      " - 6s - loss: 0.4868 - acc: 0.7779 - val_loss: 0.4623 - val_acc: 0.7761\n",
      "Epoch 26/50\n",
      " - 6s - loss: 0.4883 - acc: 0.7687 - val_loss: 0.4528 - val_acc: 0.7672\n",
      "Epoch 27/50\n",
      " - 6s - loss: 0.4962 - acc: 0.7618 - val_loss: 0.4585 - val_acc: 0.7642\n",
      "Epoch 28/50\n",
      " - 6s - loss: 0.4842 - acc: 0.7768 - val_loss: 0.4435 - val_acc: 0.7910\n",
      "Epoch 29/50\n",
      " - 6s - loss: 0.4890 - acc: 0.7741 - val_loss: 0.4538 - val_acc: 0.7731\n",
      "Epoch 30/50\n",
      " - 6s - loss: 0.4931 - acc: 0.7620 - val_loss: 0.4700 - val_acc: 0.7433\n",
      "Epoch 31/50\n",
      " - 6s - loss: 0.4878 - acc: 0.7678 - val_loss: 0.4596 - val_acc: 0.7791\n",
      "Epoch 32/50\n",
      " - 6s - loss: 0.4947 - acc: 0.7741 - val_loss: 0.4739 - val_acc: 0.7821\n",
      "Epoch 33/50\n",
      " - 6s - loss: 0.4888 - acc: 0.7787 - val_loss: 0.4485 - val_acc: 0.8090\n",
      "Epoch 34/50\n",
      " - 6s - loss: 0.4958 - acc: 0.7691 - val_loss: 0.4664 - val_acc: 0.7701\n",
      "Epoch 35/50\n",
      " - 6s - loss: 0.4925 - acc: 0.7728 - val_loss: 0.4583 - val_acc: 0.7761\n",
      "Epoch 36/50\n",
      " - 6s - loss: 0.4888 - acc: 0.7711 - val_loss: 0.4667 - val_acc: 0.8030\n",
      "Epoch 37/50\n",
      " - 6s - loss: 0.4855 - acc: 0.7777 - val_loss: 0.4574 - val_acc: 0.7910\n",
      "Epoch 38/50\n",
      " - 6s - loss: 0.4909 - acc: 0.7741 - val_loss: 0.4413 - val_acc: 0.8030\n",
      "Epoch 39/50\n",
      " - 7s - loss: 0.4875 - acc: 0.7736 - val_loss: 0.4433 - val_acc: 0.7731\n",
      "Epoch 40/50\n",
      " - 7s - loss: 0.4965 - acc: 0.7647 - val_loss: 0.4329 - val_acc: 0.8060\n",
      "Epoch 41/50\n",
      " - 6s - loss: 0.4852 - acc: 0.7777 - val_loss: 0.4439 - val_acc: 0.8000\n",
      "Epoch 42/50\n",
      " - 7s - loss: 0.4908 - acc: 0.7758 - val_loss: 0.4330 - val_acc: 0.8179\n",
      "Epoch 43/50\n",
      " - 6s - loss: 0.4858 - acc: 0.7771 - val_loss: 0.4486 - val_acc: 0.7910\n",
      "Epoch 44/50\n",
      " - 6s - loss: 0.4900 - acc: 0.7768 - val_loss: 0.4363 - val_acc: 0.7851\n",
      "Epoch 45/50\n",
      " - 6s - loss: 0.4898 - acc: 0.7794 - val_loss: 0.4459 - val_acc: 0.7910\n",
      "Epoch 46/50\n",
      " - 6s - loss: 0.4879 - acc: 0.7820 - val_loss: 0.4391 - val_acc: 0.8000\n",
      "Epoch 47/50\n",
      " - 6s - loss: 0.4753 - acc: 0.7903 - val_loss: 0.4488 - val_acc: 0.7881\n",
      "Epoch 48/50\n",
      " - 6s - loss: 0.4867 - acc: 0.7694 - val_loss: 0.4423 - val_acc: 0.7910\n",
      "Epoch 49/50\n",
      " - 6s - loss: 0.4846 - acc: 0.7744 - val_loss: 0.4559 - val_acc: 0.7940\n",
      "Epoch 50/50\n",
      " - 6s - loss: 0.4886 - acc: 0.7755 - val_loss: 0.4602 - val_acc: 0.7761\n"
     ]
    }
   ],
   "source": [
    "# Model fitting\n",
    "model_lstm = fit_lstm(X_train, Y_train, X_val, Y_val, epochs = 50, batch_size = 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nywuFc865vP4HOEs53rNDh+wAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20766040ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAg0AAAIBCAYAAADK2ozvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQiwaJElSIf8f5KM5vxMjCTkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2070eea3be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>serial</th>\n",
       "      <th>split</th>\n",
       "      <th>true_y</th>\n",
       "      <th>pred_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>567403</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.792220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>573854</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.119861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>577824</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.485571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>593504</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.181170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>598961</td>\n",
       "      <td>Validation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.336047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>609041</td>\n",
       "      <td>Validation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.853749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>609101</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.222492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>611233</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.125600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>612293</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.116665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>617451</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.073494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>621573</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.119527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>628081</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.217595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>628731</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.247399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>628791</td>\n",
       "      <td>Validation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.187659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>629491</td>\n",
       "      <td>Validation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.232895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>630194</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.248075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>631444</td>\n",
       "      <td>Validation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.868662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>638332</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.102384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>642451</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.259469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>642663</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.119698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>643451</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.116409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>649501</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.247520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>650271</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.116665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>650311</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.202849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>650482</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.506113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>652681</td>\n",
       "      <td>Validation</td>\n",
       "      <td>1</td>\n",
       "      <td>0.376194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>653561</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>655662</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.119720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>655791</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.253998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>656162</td>\n",
       "      <td>Validation</td>\n",
       "      <td>0</td>\n",
       "      <td>0.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6666</th>\n",
       "      <td>1282166</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "      <td>0.817112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6667</th>\n",
       "      <td>1282256</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>0.116810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6668</th>\n",
       "      <td>1282436</td>\n",
       "      <td>Test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.147951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6669</th>\n",
       "      <td>1282496</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>0.224179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6670</th>\n",
       "      <td>1285046</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "      <td>0.777904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6671</th>\n",
       "      <td>1286404</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>0.115295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6672</th>\n",
       "      <td>1288963</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "      <td>0.732593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6673</th>\n",
       "      <td>1289216</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "      <td>0.871530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6674</th>\n",
       "      <td>1290265</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "      <td>0.782781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6675</th>\n",
       "      <td>1290575</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>0.102215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6676</th>\n",
       "      <td>1290626</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>0.175481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6677</th>\n",
       "      <td>1291715</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>0.247399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6678</th>\n",
       "      <td>1293125</td>\n",
       "      <td>Test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.257380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6679</th>\n",
       "      <td>1294085</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "      <td>0.074539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6680</th>\n",
       "      <td>1294736</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "      <td>0.287936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6681</th>\n",
       "      <td>1297712</td>\n",
       "      <td>Test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.211781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6682</th>\n",
       "      <td>1297712</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>0.211781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6683</th>\n",
       "      <td>1298246</td>\n",
       "      <td>Test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.393674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6684</th>\n",
       "      <td>1298246</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "      <td>0.393674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6685</th>\n",
       "      <td>1298386</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>0.253998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6686</th>\n",
       "      <td>1298386</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>0.253998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6687</th>\n",
       "      <td>1303846</td>\n",
       "      <td>Test</td>\n",
       "      <td>0</td>\n",
       "      <td>0.189916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6688</th>\n",
       "      <td>1303846</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>0.189916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6689</th>\n",
       "      <td>1306076</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>0.172922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6690</th>\n",
       "      <td>1309346</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "      <td>0.287936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6691</th>\n",
       "      <td>1311386</td>\n",
       "      <td>Test</td>\n",
       "      <td>1</td>\n",
       "      <td>0.854827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6692</th>\n",
       "      <td>1311386</td>\n",
       "      <td>Train</td>\n",
       "      <td>1</td>\n",
       "      <td>0.854827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6693</th>\n",
       "      <td>1322366</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>0.303199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6694</th>\n",
       "      <td>1322366</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>0.303199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6695</th>\n",
       "      <td>1324936</td>\n",
       "      <td>Train</td>\n",
       "      <td>0</td>\n",
       "      <td>0.081920</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6696 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       serial       split  true_y    pred_y\n",
       "0      567403  Validation       0  0.792220\n",
       "1      573854  Validation       0  0.119861\n",
       "2      577824  Validation       0  0.485571\n",
       "3      593504  Validation       0  0.181170\n",
       "4      598961  Validation       1  0.336047\n",
       "5      609041  Validation       1  0.853749\n",
       "6      609101  Validation       0  0.222492\n",
       "7      611233  Validation       0  0.125600\n",
       "8      612293  Validation       0  0.116665\n",
       "9      617451  Validation       0  0.073494\n",
       "10     621573  Validation       0  0.119527\n",
       "11     628081  Validation       0  0.217595\n",
       "12     628731  Validation       0  0.247399\n",
       "13     628791  Validation       1  0.187659\n",
       "14     629491  Validation       1  0.232895\n",
       "15     630194  Validation       0  0.248075\n",
       "16     631444  Validation       1  0.868662\n",
       "17     638332  Validation       0  0.102384\n",
       "18     642451  Validation       0  0.259469\n",
       "19     642663  Validation       0  0.119698\n",
       "20     643451  Validation       0  0.116409\n",
       "21     649501  Validation       0  0.247520\n",
       "22     650271  Validation       0  0.116665\n",
       "23     650311  Validation       0  0.202849\n",
       "24     650482  Validation       0  0.506113\n",
       "25     652681  Validation       1  0.376194\n",
       "26     653561  Validation       0  0.081996\n",
       "27     655662  Validation       0  0.119720\n",
       "28     655791  Validation       0  0.253998\n",
       "29     656162  Validation       0  0.221000\n",
       "...       ...         ...     ...       ...\n",
       "6666  1282166       Train       1  0.817112\n",
       "6667  1282256       Train       0  0.116810\n",
       "6668  1282436        Test       0  0.147951\n",
       "6669  1282496       Train       0  0.224179\n",
       "6670  1285046       Train       1  0.777904\n",
       "6671  1286404       Train       0  0.115295\n",
       "6672  1288963       Train       1  0.732593\n",
       "6673  1289216       Train       1  0.871530\n",
       "6674  1290265       Train       1  0.782781\n",
       "6675  1290575       Train       0  0.102215\n",
       "6676  1290626       Train       0  0.175481\n",
       "6677  1291715       Train       0  0.247399\n",
       "6678  1293125        Test       0  0.257380\n",
       "6679  1294085       Train       1  0.074539\n",
       "6680  1294736       Train       1  0.287936\n",
       "6681  1297712        Test       0  0.211781\n",
       "6682  1297712       Train       0  0.211781\n",
       "6683  1298246        Test       1  0.393674\n",
       "6684  1298246       Train       1  0.393674\n",
       "6685  1298386       Train       0  0.253998\n",
       "6686  1298386       Train       0  0.253998\n",
       "6687  1303846        Test       0  0.189916\n",
       "6688  1303846       Train       0  0.189916\n",
       "6689  1306076       Train       0  0.172922\n",
       "6690  1309346       Train       1  0.287936\n",
       "6691  1311386        Test       1  0.854827\n",
       "6692  1311386       Train       1  0.854827\n",
       "6693  1322366       Train       0  0.303199\n",
       "6694  1322366       Train       0  0.303199\n",
       "6695  1324936       Train       0  0.081920\n",
       "\n",
       "[6696 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model testing\n",
    "val_lstm(sample_df, model_lstm.model, full_array_x, sample_df['y'], batch_size = 50, cutoff = 0.40,result_csv_save_name='lstm_result.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
